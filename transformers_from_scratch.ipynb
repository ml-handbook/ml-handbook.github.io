import json

# Define the notebook content
notebook_content = {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers From Scratch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Transformers are one of the most influential neural network architectures in modern machine learning. They power large language models, translation systems, image generators, and more. Unlike earlier sequence models such as RNNs or LSTMs, transformers rely entirely on attention mechanisms, allowing them to model long-range dependencies efficiently and in parallel.\n",
    "\n",
    "In this notebook, we will build a complete, minimal transformer step by step. The goal is not to optimize performance, but to deeply understand how transformers work by implementing each component ourselves.\n",
    "\n",
    "By the end, you will:\n",
    "\n",
    "* Understand self-attention intuitively and mathematically\n",
    "* Implement scaled dot-product attention\n",
    "* Build multi-head attention\n",
    "* Construct transformer encoder blocks\n",
    "* Train a tiny transformer on toy data\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "We assume basic familiarity with:\n",
    "\n",
    "* Python\n",
    "* PyTorch tensors and autograd\n",
    "* Linear algebra fundamentals (vectors, matrices, dot products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup\n",
    "\n",
    "We start with standard PyTorch imports. Everything in this notebook is self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Attention?\n",
    "\n",
    "Sequence models must answer a fundamental question:\n",
    "\n",
    "> **Which parts of the input should the model focus on when producing a representation?**\n",
    "\n",
    "RNNs process tokens sequentially, which makes long-range dependencies difficult to learn. Attention solves this by allowing every token to directly look at every other token and decide what matters.\n",
    "This is the core idea behind transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Each token produces three vectors:\n",
    "\n",
    "1.  **Query (Q):** what am I looking for?\n",
    "2.  **Key (K):** what do I contain?\n",
    "3.  **Value (V):** what information do I provide?\n",
    "\n",
    "Attention scores are computed by comparing queries with keys. These scores determine how much of each value to mix into the output.\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "The scaling factor $\\sqrt{d_k}$ stabilizes gradients for large dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.size(-1)\n",
    "    # Compute attention scores: (Batch, Seq_len, Seq_len)\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    \n",
    "    # Normalize scores to probabilities\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Weighted sum of Values\n",
    "    return weights @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it with dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, seq_len, d_model = 2, 4, 8\n",
    "Q = torch.randn(batch, seq_len, d_model)\n",
    "K = torch.randn(batch, seq_len, d_model)\n",
    "V = torch.randn(batch, seq_len, d_model)\n",
    "\n",
    "output = scaled_dot_product_attention(Q, K, V)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Layer\n",
    "\n",
    "Self-attention means Q, K, and V all come from the same input embeddings. We learn linear projections that map embeddings into Q, K, and V spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split the output of the linear layer into Q, K, V\n",
    "        Q, K, V = self.qkv(x).chunk(3, dim=-1)\n",
    "        attn = scaled_dot_product_attention(Q, K, V)\n",
    "        return self.out(attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "### Why Multiple Heads?\n",
    "\n",
    "Instead of a single attention operation, transformers use multiple attention heads. Each head can focus on different types of relationships (syntax, semantics, position, etc.).\n",
    "Each head works in a lower-dimensional space, and their outputs are concatenated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Reshape to (Batch, Seq_len, Num_heads, Head_dim) -> Transpose to (Batch, Num_heads, Seq_len, Head_dim)\n",
    "        Q = Q.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        K = K.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        V = V.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        # Compute attention per head\n",
    "        attn = scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Concatenate heads back together\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out(attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-Wise Feedforward Network\n",
    "\n",
    "Attention mixes information across tokens, but we still need non-linear transformations applied independently to each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder Block\n",
    "\n",
    "Each encoder block consists of:\n",
    "\n",
    "1.  Multi-head self-attention + residual connection\n",
    "2.  Feedforward network + residual connection\n",
    "3.  Layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x + self.attn(x))\n",
    "        x = self.ln2(x + self.ff(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Because transformers have no recurrence or convolution, we must inject position information explicitly.\n",
    "We use sinusoidal positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Transformer Encoder\n",
    "\n",
    "Finally, we combine everything into the full Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=100):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.pos(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Transformer\n",
    "\n",
    "Let's initialize the model and pass a random batch of integers (simulating token IDs) through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "model = TransformerEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=32,\n",
    "    num_heads=4,\n",
    "    d_ff=64,\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "# Batch size 2, Sequence length 10\n",
    "x = torch.randint(0, vocab_size, (2, 10))\n",
    "out = model(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have built a transformer encoder completely from scratch. While real-world models add optimizations like masking, dropout, and massive scale, the core ideas remain exactly what you've implemented here.\n",
    "\n",
    "Transformers succeed because they:\n",
    "\n",
    "* Model global dependencies efficiently\n",
    "* Scale extremely well with data and compute\n",
    "* Are simple, modular, and expressive\n",
    "\n",
    "From here, you can extend this notebook to:\n",
    "\n",
    "* Add causal masking\n",
    "* Build a decoder\n",
    "* Train on real language data\n",
    "* Implement a full GPT-style model\n",
    "\n",
    "Happy experimenting ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
